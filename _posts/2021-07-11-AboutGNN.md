---
layout:     post
title:      AboutGNN
subtitle:   学习
date:       2021-07-11
author:     simplex
header-img: img/page_back.jpg
catalog: true
mathjax:  true
tags:
    - 学习	
---

**前言**

因为准备考试，以及写数字图像处理作业等事情，摸了一个月没有写。不过写了也不会有人看😂。还是想吐槽一下数字图像处理的代码量，这是我上过的代码量最大的一门课，比课设的代码量还要大。每周500-1k行，六周每周一次，加起来可能有4k-5k行。

这个学期，学习了一些有关于图神经网络的东西。现在考完了，整理一下。也算是做了一件一直想做的事情。总觉得自己好像什么都没学23333。那就看看我到底看了多少吧。

# GCN

最先看的肯定是Kipf大神的**SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS**，他的工作主要是对于ChebyNet的改进。

## 谱图卷积（Spectrum Graph Convolution）

这部分给我印象最深刻的是一个中国科学院大学的一个老师上的课，有人录下来传到B站，我恰好看到了，不过现在好像删了，可惜可惜。那个老师用了不到一个小时，就把这部分讲得十分清楚。



首先给定一个图$$G(V,E,A)$$,其中，$$V$$表示顶点；$$E$$表示边；$$A$$表示带边权的邻接矩阵。

### 图拉普拉斯算子（Graph Laplacian）

有图拉普拉斯算子$$L=D-A$$，其中，$$D_{ii}=\Sigma_jA_{ij}$$，且$$D$$为对角阵。

对于拉普拉斯算子，在学习数字图像处理之后（没有学过数字信号处理的我留下了不学无术的泪水），我的理解是一个二阶梯度算子。实际不论他是拉普拉斯算子还是图拉普拉斯算子，都干了一件事情，就是将元素自身与邻域的均值做差分。

图拉普拉斯算子有一个很重要的公式，对于向量$$f$$


$$
f^TLf={1\over2}\sum_{i,j}(f_i-f_j)^2
$$



对称归一化的图拉普拉斯算子


$$
L_{sym} = I-D^{-{1\over 2}}AD^{-{1\over 2}}
$$


对称归一化的目的，据说能够提升卷积的效果，因为对于$$A_{ij}$$有$$D_{ii}$$和$$D_{jj}$$进行约束。下文的$$L$$都默认为$$L_{sym}$$

### 图傅里叶变换（Graph Fourier Transformation）

对于信号$$x\in R^N$$定义$$U^Tx$$为$$x$$的图傅里叶变换，其中，$$U$$为拉普拉斯算子$$L_{sym}$$的特征向量组成的矩阵

### 谱图卷积

对于信号$$x\in R^N$$和滤波器$$g_\theta=diag(\theta),\theta\in R^N$$，这是拉普拉斯算子$$L$$的特征值的对角矩阵，定义


$$
g_\theta * x=Ug_\theta U^Tx
$$


但是对于矩阵$$L_{sym}$$分解太慢$$O(N^2)$$，



## ChebyNet

为了近似求解$$Ug_\theta U^T$$，用切比雪夫多项式逼近。


$$
g_{\theta'} \star x=\sum_{k=0}^{K}\theta'_kT_k(\tilde L) x\\
=\sum_{k=0}^{K}\theta'_kL^k x\\
$$


其中，$$\tilde L = {2 \over{\lambda_{max}}}L-I$$；$$\theta_k$$为因子；$$T_k(x) = 2xT_{k-1}(x)-T_{k-2}(x),T_0(x)=I, T_1(x)=x$$

这篇论文没有仔细读，不会。

很多论文都是以这篇论文为基础做创新。



## GCN

GCN对ChebyNet做了简化，对于ChebyNet提出的式子，做了一阶近似，且令$$\theta = \theta_0=-\theta_1$$得到


$$
g_{\theta'} \star x\approx\theta(I+D^{-{1\over 2}}AD^{-{1\over 2}} )x
$$


又使用了重正则化技巧，有


$$
g_{\theta'} \star x\approx\theta(\tilde D^{-{1\over 2}}\tilde A\tilde D^{-{1\over 2}} )x
$$


其中，$$\tilde A =A +I,\tilde D_{ii} = \Sigma_{j}A_{ij}$$



最终，层间的传播公式


$$
H^{(l+1)}=\sigma (\tilde D^{-{1\over 2}}\tilde A\tilde D^{-{1\over 2}}H^{(l-1)}x)
$$



看了GCN以后，我就一个感受，就是这也太简洁（简单）了吧，这么化简，竟然有这么不可思议的效果，令人佩服。这也是我一直向往的东西——简单而非常有效方法。关于GCN那么强的表达能力，后面有很多人做了分析。



# 其他的方法

说是其他的方法，其实都是GCN的扩展，改进

## GAE & VGAE

Graph AutoEncoder & Graph Variational AutoEncoder

就是图领域的autoencoder和variational autoencoder

主要思路就是使用GCN将输入的特征$$X$$编码成低维特征$$Z$$，然后将低维特征解码得到邻接矩阵，常用的解码方式为Dot Product Decoder。如下式


$$
\hat A = \sigma(ZZ^T), Z = GCN(X,A;\Theta)
$$


损失函数就是





VGAE是一个生成式模型，主要过程与GAE类似，如下式


$$
\mu =GCN_{\mu}(X,A), log \sigma = GCN(X,A)\\
q(z_i|X,A) = \mathcal N(z_i;\mu_i,diag(\sigma_i^2))\\
q(Z|X, A) = \prod_{i=1}^Nq(z_i|X,A)\\
p(A_{ij}|z_i,z_j) = \sigma(z_i^\mathsf Tz_j)\\
p(A|Z) = \prod_{i=1}^N \prod_{j=1}^Np(A_{ij}|z_i,z_j)


\
$$


损失函数有两部分，第一部分是复现的邻接矩阵和本真的临界矩阵的误差，使用交叉熵衡量（GAE就只有这部分）；第二部分，KL散度，如下式：


$$
\mathcal L = \mathbb E_q(z_i|X,A)[log p(A|Z)]-KL[q(Z|X, A)||p(Z)]
$$



这有一个非常严重的正负例不平衡的问题，所以使用负采样，来缓解这个问题。我在实验之后，我的结论是，正负例过于不平衡，这样得到的复现邻接矩阵太大的意义。

## SGC

SGC认为GCN中的层间传播公式中，对于表达能力有贡献的主要是两部分，一部分是谱图卷积的部分，另一部分是激活函数$$\sigma(*)$$。后一部分，激活函数相对来说是不重要的，同时也为了加速，去掉了激活函数，于是有：


$$
\tilde Y_{sgc} = softmax(S^KX\Theta),S =\tilde D^{-{1\over 2}}\tilde A\tilde D^{-{1\over 2}}
$$


SGC更重要的是说明了，GCN的本质就是低通滤波，就是这个低通滤波，trigger了GCN的成功。这启发了后面的一些方法

## GALA

这是一个

接下里的部分等我夏令营搞好再写吧...
