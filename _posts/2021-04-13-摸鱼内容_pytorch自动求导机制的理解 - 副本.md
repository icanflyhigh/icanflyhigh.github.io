---
layout:     post
title:      pytorch自动求导机制的理解
subtitle:   学习
date:       2021-04-13
author:     simplex
header-img: img/page_back.jpg
catalog: true
tags:
    - 日常 
    - 学习
---

# pytorch自动求导机制的理解

先贴一段代码

```python
X = torch.tensor(list(range(3))).reshape(3, 1).float()
y = torch.tensor(list(range(3))).reshape(3, 1).float()
W = torch.ones((1, 1)).float() * 2
W.requires_grad = True
optimizer = optim.Adam([W])
out = F.mse_loss(F.linear(X, W)[:1], y[:1])
out.backward()
optimizer.step()
print(W)
```

1. 所有自动求导都是对于tensor的
2. 损失函数之后接，tensor的backward
3. tensor需要设置 requires_grad = True，才能成为计算图的叶子tensor，才能更新参数
4. optim.Adam()的第一个参数指示需要跟新参数的tensor，需要用list之类的包起来

   