---
layout:     post
title:      SfM
subtitle:   学习
date:       2021-07-11
author:     simplex
header-img: img/page_back.jpg
catalog: true
mathjax:  true
tags:
    - 学习	
---



**南京理工大学计算机科学与工程学院**

**视觉传感技术**

# 目 录

[1 实验要求 1](#_Toc75880958)

[2 SfM 1](#_Toc75880959)

[2.1 简介 1](#_Toc75880960)

[2.2 特点 1](#_Toc75880961)

[3 方法 1](#_Toc75880962)

[3.1 增量式SfM流程 1](#_Toc75880963)

> [3.1.1 关联搜索-特征提取 2](#_Toc75880964)
>
> [3.1.2关联搜索-匹配 2](#_Toc75880965)
>
> [3.1.3 关联搜索-几何验证 2](#_Toc75880966)
>
> [3.1.4 增量式重建-初始化 2](#_Toc75880967)
>
> [3.1.5 增量式重建-图像注册 3](#_Toc75880968)
>
> [3.1.6 增量式重建-三角化 3](#_Toc75880969)
>
> [3.1.7 光束法平差 3](#_Toc75880970)
>
> [3.1.8 挑战 3](#_Toc75880971)

[3.2 论文的贡献 3](#_Toc75880972)

> [3.2.1 场景图增强 3](#_Toc75880973)
>
> [3.2.2 下一最好视图的选择 4](#_Toc75880974)
>
> [3.2.3 鲁棒且高效的三角化 5](#_Toc75880975)
>
> [3.2.4 光束法平差 6](#_Toc75880976)
>
> [3.2.5 冗余视图最少化 6](#_Toc75880977)

[4 实验 7](#_Toc75880978)

[4.1实验设置 7](#_Toc75880979)

[4.2 实验流程 7](#_Toc75880980)

> [4.2.1 创建工程 7](#_Toc75880981)
>
> [4.2.2 进行3D重建 8](#_Toc75880982)

[4.3 实验分析与总结 15](#_Toc75880983)

[5 总结 16](#_Toc75880984)

[6 附录 16](#_Toc75880985)

[6.1 SIFT 16](#_Toc75880986)

> [6.1.1 尺度空间极值检测 16](#_Toc75880987)
>
> [6.1.2 特征点定位 16](#_Toc75880988)
>
> [6.1.3 方向确定 17](#_Toc75880989)
>
> [6.1.4 特征点描述 17](#_Toc75880990)
>
> [6.1.5 SIFT特征的匹配 17](#_Toc75880991)

[6.2 单应矩阵、本质矩阵、基础矩阵 17](#_Toc75880992)

> [6.2.1 单应矩阵 17](#_Toc75880993)
>
> [6.2.2 本质矩阵 18](#_Toc75880994)
>
> [6.2.3 基础矩阵 19](#_Toc75880995)
>
> [6.2.4 单应矩阵和基础矩阵的区别 19](#_Toc75880996)

[6.3 Perspective-n-Point 20](#_Toc75880997)

[6.4 LDT 20](#_Toc75880998)

[6.5 参考 21](#_Toc75880999)

 

# <span id = "_Toc75880958" ></span> 1 实验要求

阅读一篇SFM较完整系统的论文，实现一个SFM系统，采集实验数据运行系统，并分析实验过程和结果。

# <span id = "_Toc75880959" ></span>2 SfM

**SfM：Structure From Motion运动恢复结构**

## <span id = "_Toc75880960" ></span>2.1 简介

计算机视觉中的运动恢复结构 (SfM) 问题:对一组投影测量（二维（2D）图像集合），通过估计其中图像对应的相机运动，来恢复静止场景的三维（3D）结构的问题。其中，投影测量表示。本质上，SfM 涉及三个主要阶段：（1）提取图像中的特征（例如，关键点、线条等）并在图像之间匹配这些特征；（2）相机运动估计（例如，使用相对成对相机从提取的特征估计的位置）；（3）使用估计的运动和特征恢复 3D 结构（例如，通过最小化重投影误差）。

主要分为3类: 增量式(increment)、分层式(hierarchical)、全局式(global)

## <span id = "_Toc75880961" ></span>2.2 特点

相对于即时定位与地图构建（SLAM），有精度高、规模大、动态化重建的特点。

相对于使用高成本的激光扫描设备为地形或建筑目的捕获大型点云， SfM与多视图立体匹配（MVS）相结合的方法具有低成本的特点。

# <span id = "_Toc75880962" ></span>3 方法

论文名：**Structure-from-Motion Revisited**

文章采用了增量式SfM的方法，相较于以前的方法，系统的鲁棒性、准确性、完整性和可扩展性都得到了巨大提升。

## <span id = "_Toc75880963" ></span>3.1 增量式SfM流程

主要分为两大步骤：（1）关联搜索；（2）增量式重建。

其中关联搜索寻找有重叠场景的图像。输入为原始图像集，输出为经过几何验证的图像对（场景图）、地图点对应的图像投影点。由以下步骤构成：（a）特征提取；（b）匹配；（c）几何验证。

![](D:/HomePage/icanflyhigh.github.io/_posts/media/image2.jpg)(width="6.102083333333334in" height="1.304861111111111in")增量式重建输入为场景图，输出为注册的图像位姿（pose）的估计和作为一组点的重建场景结构。由以下步骤构成：（a）初始化；（b）图像注册（c）三角化（d）光束法平差。

### <span id = "_Toc75880964" ></span>3.1.1 关联搜索-特征提取

输入，对于中的每个图像，SfM提取特征得到局部特征集合，其中，表示特征点的位置。表示外观描述符（an appearance descriptor）特征具有辐射和几何不变性， 这使得SfM 可在多个图像中唯一地识别它们。

特征提取一般采用尺度不变特征变换（SIFT），及其衍生算法如加速稳健特征（SURF）。论文中采用了SIFT算法。

关于SIFT特征提取的详细介绍，参见附录6.1

### <span id = "_Toc75880965" ></span>3.1.2关联搜索-匹配

SfM通过利用作为图像的特征，来匹配拥有相同场景部分的图像。

一个简单的方法是通过使用相似性比较方法比较，来检测直接检测图像集sI图像两两间最相似的地方。这种方法的复杂度非常高，不适合与大数据集。其它的方法还有如顺序匹配法、空间匹配、词汇树匹配法等。

输出一个潜在的匹配图像对和匹配图像之间的相关特征的对应关系。

### <span id = "_Toc75880966" ></span>3.1.3 关联搜索-几何验证

由于匹配仅基于外观（Appearance），不能保证相应的特征实际映射到相同的场景点。 SfM估计一个变换，该变换是通过投影几何得到的在图像对的对应特征点之间的映射，来验证匹配。根据图像对不同的空间构型计算不同的几何关系：单应矩阵（纯旋转或平面特征）捕获到的平面场景的纯旋转或者纯平移的摄像机的转换；本质矩阵（已标定）和基础矩阵（未标定）描述运动中相机的关系，并且可以通过三焦点张量扩展到三个视图。由于匹配的对应关系经常受到外点（outlier）的干扰，因此需要比如样本一致性（RANSAC）算法等的技术，鲁棒地估计参数。如果估计的变换在图像对之间映射了足够多的特征，则认为图像对通过了几何验证。

这一步的输出（即关联搜索的输出）为经过验证的图像对，及其对应的内点，也可包含图像间几何关系。该阶段输出也被称为场景图（scene graph），图像为顶点，关联关系为边。

关于单应矩阵、本质矩阵、基础矩阵的详细介绍，参见附录6.2

### <span id = "_Toc75880967" ></span>3.1.4 增量式重建-初始化

SfM使用精心选择的两视图来初始化重建模型。选择合适的初始图像对至关重要，因为重建可能永远无法从错误的初始化中恢复正常。

在有较多重叠图像的位置初始化使重建结果更鲁邦准确。而在较少重叠图像初初始化，将使运行速度较快。

### <span id = "_Toc75880968" ></span>3.1.5 增量式重建-图像注册

新图像可以通过使用特征对应于已经三角化的点来求解 Perspective-n-Point (PnP) 问题来注册到当前图像。Perspective- n- Point 是在给定一组n个3D 点及其在图像中相应的 2D 投影的情况下，估计校准相机的位姿（pose）和内参的问题。由于外点的影响，使用RANSAC等方法，来鲁棒地估计相机的姿态。

关于PnP问题的详细介绍，参见附录6.3

下一张图像选择的详细介绍，参见3.2.2

### <span id = "_Toc75880969" ></span>3.1.6 增量式重建-三角化

三角化是指在 3D 空间中确定一个点到两个或更多图像上的投影的过程。

新注册的图像可以通过三角化增加场景点，扩展场景覆盖范围，只要至少有一个不同的视点被注册图像也覆盖了新的场景部分。

三角化是 SfM 中的关键步骤，因为它通过冗余增加了现有模型的稳定性，并提供额外的 2D-3D 对应关系来提供新图像的注册。

三角化的方法有很多，比如：中点法、直接线性变换法、本质矩阵法等

论文中三角化的详细介绍，参见3.2.3

### <span id = "_Toc75880970" ></span>3.1.7 光束法平差

图像注册和三角化是两个分离过程，相机姿态的不确定性会传播到三角点，反之亦然。额外的三角化可以通过增加冗余来改善初始相机姿态，增加鲁棒性。

光束法平差通过最小化重投影误差，联合非线性调整相机参数和点参数。

光束法平差的详细介绍，参见3.2.4

### <span id = "_Toc75880971" ></span>3.1.8 挑战

SfM算法在建图完整性，鲁棒性等方面经常会产生如，漏注册、错误注册、累计误差等的问题。原因在于，关联搜索中由于近似匹配产生了不完整的场景图，没有足够的连通性，且影响精度；由于场景结构的丢失或错误,导致重构阶段的失败。图像注册与三角化又是一个相互捆绑的问题。

## <span id = "_Toc75880972" ></span>3.2 论文的贡献

论文的贡献有以下几点：

场景图增强（Scene Graph Augmentation）、下一最好视图的选择（Next Best View Selection）、鲁棒且高效的三角化（Robust and Efficient Triangulation）、光束法平差(Bundle Adjustment)、冗余视图最少化（Redundant View Mining）。

### <span id = "_Toc75880973" ></span>3.2.1 场景图增强

正如之前提到的，使用几何验证来增加场景图。一个已经增强的场景图能够有效地找到三维重建的最佳初始化。

首先估计基础矩阵F，如果内点数（）较多，则通过验证。

对同一图像对，计算单应矩阵H下的内点数（）。为了近似如GRIC模型选择方法，如果，认为相机在一般场景中移动（以纯旋转假设估计效果不好）。对于标定的图像，估计本质矩阵，计算内点数（）。如果，认为标定正确。如果标定正确，且相机发生了位移，分解本质矩阵，三角化相应内点，并且计算夹角 ，该夹角可以区分纯旋转和平面特征。

由于网上的照片经常含的水印，时间戳，边框(WTF图像)等错误地链接了不同位视点的图像。我们通过估计相似变换下图像四周上的内点数。任何具有的图像对都被认为是WTF图像，不会被加入场景图。对于有效的图像对，在场景图使用模型类型（一般、纯旋转、平面）标记以及最好模型对应的内点。

在初始化时选择标定好的，非纯旋转的图像对。对于纯旋转，建图过程中并不进行三角化，以增加三角化和图像注册的鲁棒性。

### <span id = "_Toc75880974" ></span>3.2.2 下一最好视图的选择

下一图像对选择至关重要，因为每一个决定影响剩余的重建。一个错误的决定可能会导致一连串的相机配准错误和三角化错误。严重影响位姿的估计与三角化的精度。

一般的方法是选取看到最多地图点的图像，目的是最大限度地减少相机切除的不确定性。

实验证明PnP的精度依赖于地图点的数量以及其分布情况。论文采用的是多分辨率分析方法。

下一个最佳视图的候选图像同时可以看到至少个地图点的所有未被注册的图像。在候选图像中记录地图点数目及其分布。数目较多且分布均匀的将优先选择。

将每张图像划分网格，在两个维度上都有个单元，每个格子用空和满来表示。当某一格子中当点变为可见时，该格子标记为满，且该图像点评分按权重增大。通过使用更高的分辨率对图像进行分区，将前面描述的方法扩展到具有级的多分辨率金字塔。每个连续级别的分辨率。分数在所有级别上以分辨率相关的权重累积。该数据结构及其分数可以在线有效更新。L = 3 时，图像中不同分布（顶部和底部）的不同点数（左侧和右侧）的得分如下图

![](D:/HomePage/icanflyhigh.github.io/_posts/media/image27.jpg)(width="4.7243055555555555in" height="1.9041666666666666in")

### <span id = "_Toc75880975" ></span>3.2.3 鲁棒且高效的三角化

双目几何极线上特征点错误匹配，不正确的相机位姿会导致出现大量外点。

论文提出的三角化方法利用传递性建立大基线影像之间的对应点关系，可以稳健地估计受外点污染的特征轨迹中的所有点，同时有较小的计算成本。

使用了RANSAC来处理任意程度外点污染情况下的多视图三角化问题。

设特征轨迹（feature track），具有一定的外点率，其中𝑇𝑖包括归一化后的像平面坐标，对应的相机位姿，表示从世界坐标系到相机坐标系的变换。目标是是最大会满足下式表示的条件良好的双目三角化

其中表示三角化方式（论文采用了线性三角化（DLT）法），表示空间已经三角化的点；

一个条件良好双目三角化模型满足以下两个约束

1.  有足够的三角化角度（交会角）

第二、关于视点的深度是正的，其中深度由下式定义

其中，表示的第m行，第n列的元素。

同时，三维点在影像上的重投影误差小于设定的阈值，重投影误差如下式

RANSAC迭代最大化目标函数，并且通常它随机均匀地对两个最小集合进行采样。然而，由于对于小的可能会多次采样相同的最小集合，使随机采样为仅生成唯一的样本。为了确保 η 至少采样了一个无外点的最小集合，RANSAC 必须至少运行 K 次迭代。 由于先验内点比率未知，我们将其设置为一个小的初始值 并在我们找到更大的共识集时调整 K（自适应停止标准）。 因为一个特征轨迹可能包含多个独立的点，我们通过从剩余的测量中删除共识集来递归地运行这个过程。如果最新共识集的大小小于 3，则递归停止。

线性三角化法LDT的介绍，参见附录6.4

### <span id = "_Toc75880976" ></span>3.2.4 光束法平差

为了减轻累积误差，在图像注册和三角化后执行光束法平差（BA）。其中，在每次图像注册后，对连接最强的图像集执行局部 BA；在模型增长一定百分比后才执行全局 BA。

#### 3.2.4.1 参数化（Parameterization）

考虑到潜在的异常值，使用柯西函数作为局部 BA的损失函数。 对于几百个相机的问题，我们使用稀疏直接求解器；对于更大的问题，使用共轭梯度法（PCG）。 平差过程采用了Ceres Solver库，还可以选择任意数量图像间是否共享相机模型。对于无序的互联网照片，使用具有一个径向畸变参数的简单相机模型。

#### 3.2.4.2 过滤（Filtering）

BA后过滤掉重投影误差较大的模型；

对于每个点，每个地图点满足最小三角化角；

检查退化的相机，这些相机是指异常视场角、畸变系数异常大，需要滤去。要求在光束法平差中焦距和畸变参数一起参与优化，同时为了解决像主点校正这个病态问题，对于未校正的相机，固定像主点为像片中心点

#### 3.2.4.3 再三角化（Re-Triangulation）

在全局光束法平差之前，进行再三角化操作；

全局光束法平差之后，针对光束法平差前由于估计的错误位姿导致的三角化失败的点进行再三角化操作，以提高重建场景的完整性。这些点的观测值误差小于外点过滤中对应阈值。

#### 3.2.4.4 迭代调整（Iterative Refinement）

BA、再三角化、过滤迭代执行，直到滤除点和再三角化点较少；

### <span id = "_Toc75880977" ></span>3.2.5 冗余视图最少化

BA 是 SfM 的主要性能瓶颈。

论文的方法利用增量 SfM 和密集图像集的固有特征，通过将冗余相机聚类为场景高度重叠组来更有效地参数化 BA。首先，利用SfM固有属性将问题划分为内部参数被分解的子图；将场景分割成许多小的，高度重叠的组，然后每个组由一个相机替代；

对于具有点的场景，每个图像都可以用二值的可见性向量 来描述，其中，如果点在图像 i 中可见，则中的第 n 个元素为 1，否则为 0。 图像 a 和 b 之间的交互程度是通过对它们的向量进行按位运算来计算，如下式

将图像按照看到的地图点数量排序，构建图像组时，从中选择第一张图像，以及使 最大的。如果则将加入，同时移出。否则创建新组。为了加快的查找，参考空间最近邻的思想，找到影像±度视角范围内的图像作为候选。

每组的B目标函数变为

# <span id = "_Toc75880978" ></span>4 实验

## <span id = "_Toc75880979" ></span>4.1实验设置

软件设置：采用开源3D重建库colmap、三维点云查看软件meshlab

数据设置：采用图像数据集SceauxCastle 、自己拍摄的图像数据

在自己拍摄图像数据时，需要选择尽量大的场景，并且场景光线足够好，以保证有足够的特征点进行匹配，能重建足够多的三维点。

## <span id = "_Toc75880980" ></span>4.2 实验流程

## # <span id = "_Toc75880981" ></span>4.2.1 创建工程

打开colmap，选择file-\>new project，通过创建新的数据库，和选择图像集的文件夹来新建项目，新建成功后，会有一个.ini的项目文件

![](D:/HomePage/icanflyhigh.github.io/_posts/media/image63.jpeg)(width="4.636805555555555in" height="2.6416666666666666in")

![](D:/HomePage/icanflyhigh.github.io/_posts/media/image64.jpg)(width="3.902083333333333in" height="2.779166666666667in")原图像集中的一张图像

### <span id = "_Toc75880982" ></span>4.2.2 进行3D重建

在processing中选择feature extraction来提取sift特征

这里相机选择了SIMPLE_RADIAL，表示相机参数未知，图片可能有一定失真

![](D:/HomePage/icanflyhigh.github.io/_posts/media/image65.jpeg)(width="4.01875in" height="2.323611111111111in")

特征图

![](D:/HomePage/icanflyhigh.github.io/_posts/media/image66.jpg)(width="5.281065179352581in" height="3.9607983377077867in")

![](D:/HomePage/icanflyhigh.github.io/_posts/media/image67.jpeg)(width="3.7631944444444443in" height="2.176388888888889in")在processing中选择feature matching 来提取进行sift特征匹配，同时进行了几何验证，并且获得场景图，选择了完全匹配（就是前文提到的简单的遍历匹配的方法），因为图像数量比较少，只有18张，而且算力充足。

![](D:/HomePage/icanflyhigh.github.io/_posts/media/image68.jpeg)(width="5.252083333333333in" height="1.8736111111111111in")特征匹配示例

就此，完成了增量式SfM的关联搜索步骤

在reconstruction中选择start reconstruction 完成3D重建

重建时每个步骤的参数如下图

####   初始化

  ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image69.jpeg)(width="4.859397419072616in" height="2.9652996500437445in")
  图像注册
  ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image70.jpeg)(width="6.102083333333334in" height="1.9902777777777778in")
  三角化
  ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image71.jpeg)(width="6.102083333333334in" height="3.004861111111111in")
  光束法平差
  ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image72.jpeg)(width="6.102083333333334in" height="5.2444444444444445in")
  过滤
  ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image73.jpeg)(width="6.102083333333334in" height="1.9090277777777778in")

相机的位姿和三维点如图所示，相机位姿与实际比较，发现SfM得到的位姿是准确的

![](D:/HomePage/icanflyhigh.github.io/_posts/media/image74.jpeg)(width="6.102083333333334in" height="3.682638888888889in")将三维点保存后，使用meshlab进行更进一步地观察

![](D:/HomePage/icanflyhigh.github.io/_posts/media/image75.jpeg)(width="4.788194444444445in" height="3.201388888888889in")

可以看出小区外围的树丛，以及小区的中的凳子被部分重建了出来

再使用多视图立体几何（Multi-View Stereo MVS）的方法进行稠密重建得到下图，并且经过融合（fusion），得到下图

![](D:/HomePage/icanflyhigh.github.io/_posts/media/image76.jpeg)(width="6.102083333333334in" height="4.092361111111111in")

可以看到通过SfM和MVS，场景被比较完好的复现了出来。

再尝试其他的数据集，得到的结果如下

  原图         ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image77.jpeg)(width="2.712933070866142in" height="2.038636264216973in")    ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image78.jpeg)(width="2.694005905511811in" height="1.9998873578302712in")

------------ ------------------------------------------------------------------------------------ -------------------------------------------------------------------------------------

  特征图       ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image79.jpeg)(width="2.7125in" height="2.016392169728784in")               ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image80.jpeg)(width="2.69375in" height="1.999695975503062in")
  匹配图       ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image81.jpeg)(width="2.454747375328084in" height="0.9179811898512686in")   ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image82.jpeg)(width="2.760475721784777in" height="1.0263429571303586in")
  重建图       ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image83.jpeg)(width="2.600012029746282in" height="2.080009842519685in")    ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image84.jpeg)(width="2.88501312335958in" height="1.8895898950131234in")
  稠密重建图   ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image85.jpeg)(width="2.7390791776028in" height="1.965703193350831in")      ![](D:/HomePage/icanflyhigh.github.io/_posts/media/image86.jpeg)(width="3.0407305336832895in" height="2.0112357830271215in")

可以看到，右侧的水槽复原不是很好，主要原因就是水槽壁过于光滑，还有反光，特征点太少，不同角度观察反光不同，导致水槽壁特征点匹配也比较困难。这是SfM的一个缺点。使用SIFT获取特征点，具有局限性。

## <span id = "_Toc75880983" ></span>4.3 实验分析与总结

通过实验，可以看出，论文中提出的方法，对于三维重建非常有效。即使输入的图像集很小只有十几张图像，也能够比较好地得到相机的位姿以及三维点。在使用MVS等方法之后，能到的非常好的三维场景，堪比现实场景。

不过，如上文所述，SfM也存在着一些缺点。对于小场景，光线暗，光滑表面的场景匹配存在问题。这个问题的原因是SIFT算法的局限性。需要调整的参数非常多，这一方面影响了系统的鲁棒性，另一方面加大了调试中参数搜索的成本。系统在三维点少的时候，表现不是很理想。

![](D:/HomePage/icanflyhigh.github.io/_posts/media/image87.jpeg)(width="4.419444444444444in" height="2.3826388888888888in")我认为，对于这些问题，可以从以下路线进行解决。（1）针对图像质量差的问题，可以在处理之前，对图像进行预处理，比如说通过直方图归一化等的图像增强手段使图像质量提升（2）使用卷积神经网络代替SIFT，神经网络具有极强的表达能力，能够提取出SIFT无法表达的特征。同时神经网络也可以应用在系统的其他方面，由于通过神经网络端到端的特性，可以大幅度减少待调试的参数量。（3）对于场景图，增量式这一路上复杂的手段得到三维场景，这可以看成是一个生成式模型。所以，可以使用对抗式生成神经网络（GAN），来生成三维场景。对于输入场景图，一般的神经网络不能很好利用边的连接信息，所以需要使用图神经网络（GNN）进行处理。综上，我提出的框架如下图所示：

# <span id = "_Toc75880984" ></span>5 总结

通过这次实验，我通过仔细阅读论文，以及大量查阅网上文章，我对运动恢复结构有了较深入的理解，明白了每一步在做什么，为什么这么做，以及从数学以及实际操作的角度，理解了每个步骤的意义。通过开源库colmap，我简单地实现了三维重建，在细节上，对于SfM有了更加深入的了解。在撰写报告的过程中，我阅读了部分colmap的代码，对于SfM的c++实现，有了一定的认识。我的阅读能力，动手能力，写作能力得到了提升。

# <span id = "_Toc75880985" ></span>6 附录

## <span id = "_Toc75880986" ></span>6.1 SIFT

SIFT: The scale-invariant feature transform 尺度不变特征变换，用于检测和表述图像中的局部特征。

特点：

（1）即使在有比较大的噪声和部分遮挡下也可以识别对象，因为 SIFT 特征描述对均匀缩放、方向、光照变化是不变的，并且对仿射失真部分不变。

（2）模仿人的感受野的变化

（3）区分性好，能够在海量特征数据库中进行快速准确的区分信息进行匹配

（4）多量性，就算只有单个物体，也能产生大量特征向量

（5）高速性，能够快速的进行特征向量匹配

（6）可扩展性，能够与其它形式的特征向量进行联合

SIFT算法有以下步骤：（1）尺度空间极值检测；（2）特征点定位；（3）方向确定（4）特征点描述

### <span id = "_Toc75880987" ></span>6.1.1 尺度空间极值检测

首先检测感兴趣的点，这些点在 SIFT 框架中被称为特征点。SIFT使用高斯差分金字塔(Difference of Gaussians DoG)来的极大或极小值作为特征点。

DoG图像定义,

是原图像高斯模糊之后的结果

最后一个参数，是尺度空间的参数

高斯金字塔和DoG有以下特征点：

由所在组（octave）以及所在组中的层数决定

DoG由原本高斯金字塔的组（octave），通过在组内相邻层做差分得到

极值检测

每个像素点要和其同一σ的像素以及相邻层的像素进行比较。如果像素值是所有比较像素中的最大值或最小值，则将其选为候选特征点。

### <span id = "_Toc75880988" ></span>6.1.2 特征点定位

主要就是剔除一些不好的特征点。最初的方法是在候选特征点的位置和尺度上定位每个特征点。新方法计算极值的插值位置，提高来匹配性和稳定性。

剔除不好点的关键:

丢弃低对比度的特征点

消除边缘效应

### <span id = "_Toc75880989" ></span>6.1.3 方向确定

对于极值点，对应的高斯金字塔中的位置的

由公式

得到特征点的方向和模

然后使用直方图统计特征点邻域(3×1.5σ范围)内像素对应的梯度方向和幅值。梯度方向的直方图的横轴是梯度方向的角度（梯度方向的范围是0到360度，8柱，10柱，36柱都可以），纵轴是梯度方向对应梯度幅值的累加，在直方图的峰值就是特征点的主方向，保留峰值大于主方向峰值80％的方向作为该特征点的辅方向。

得到特征点的主方向后，对于每个特征点可以得到三个信息，即位置、尺度和方向。由此可以确定一个SIFT特征区域，中心表示特征点位置，半径表示关键点的尺度，箭头表示主方向，以及辅方向。

### <span id = "_Toc75880990" ></span>6.1.4 特征点描述

下面就需要使用一组向量（KeyPoint Descriptor）来描述特征点

对于特征点,要以特征点为中心，在附近邻域内将坐标轴旋转θ，旋转后以主方向为中心取 16×16的窗口，分成4×4个子区域。在最后在每个4×4的子区域上绘制8个方向的梯度直方图，计算每个梯度方向的累加值，即可形成一个种子点。最终，得到总共有16×8=128维的特征向量。

### <span id = "_Toc75880991" ></span>6.1.5 SIFT特征的匹配

对于输入的两幅图像分别为图像1和图像2，使用上述算法从输入图像中获得 SIFT 特征。

一种简单的2最近邻（2NN）的做法是对于图像2的特征点，使用基于欧几里德距离的最近邻方法来匹配图1中的两个特征点。对于最近邻距离与第二近邻距离之比大于 d的那些关键点，将拒绝匹配，其中d通常取0.8。然后就完成了匹配。

其他的方法还有：近似最近邻（approximate nearest neighbor）、层次聚类（hierarchical clustering）、局部敏感哈希（locality sensitive hashing）。

## <span id = "_Toc75880992" ></span>6.2 单应矩阵、本质矩阵、基础矩阵

### <span id = "_Toc75880993" ></span>6.2.1 单应矩阵

单应矩阵 (Homography)，约束了同一3D空间点在两个像素平面的2D齐次坐标。

其中，表示单应矩阵约束了和的方向是同方向。通过叉乘计算消去齐次的尺度因子有

因为和同方向，所以其叉乘结果为向量。

依据推导可得，单应矩阵由两相机旋转和平移信息，两相机内参矩阵，平面参数组成：

### <span id = "_Toc75880994" ></span>6.2.2 本质矩阵

本质矩阵反映了空间一点P的像点在不同视角摄像机下摄像机坐标系中的表示之间的关系。

对于两个相机位置相机1和相机2，观测点P在相机1坐标系就可以通过刚体转化变成相机2坐标

其中，和分别表示旋转和平移

于是

其中， 表示对极平面的法线，左点乘一个P\'有

由于P\'与法线是垂直的，有

两向量的叉乘可以转换为一向量的反对称矩阵与另一向量的点乘，于是

表示T的反对称矩阵，令 ,则有

其中，E就是本质矩阵.

### <span id = "_Toc75880995" ></span>6.2.3 基础矩阵

基础矩阵反映空间一点P的像素点在不同视角摄像机下图像坐标系中的表示之间的关系。

相机矩阵分为内参和外参的，代数表示为

其中，和 分别表示内参矩阵和外参矩阵，和分别表示图像点和世界点。令外参矩阵转换到相机坐标系的世界点为，则有

考虑左右两视图，由于K是可逆的，有

其中，为本质矩阵

代入有

令，则有

其中，就是基础矩阵

### <span id = "_Toc75880996" ></span>6.2.4 单应矩阵和基础矩阵的区别

（1）使用场景

基础矩阵表示的是两视图的对极约束，和三维场景的结构无关，只依赖于相机的内参数以及外参数，需要两个相机的位置有旋转和平移。

单应矩阵对场景的三维结构有了更多的要求，需要场景中的点在同一个平面上； 或者是，对相机的位姿有了要求，两个相机之间只有旋转而无平移。

（2）约束关系

基础矩阵表示的像点和另一幅图像上的对极线的映射关系，使用基础矩阵无法得到像点对应点在另一幅图像上的确切位置。

单应矩阵则是点和点的映射，使用单应矩阵可以找到像点在另一幅图像上对应点的确切位置。

（3）使用单应矩阵而不是基础矩阵

相机只有旋转而无平移的时候，两视图的对极约束不成立，基础矩阵F为零矩阵，这时候需要使用单应矩阵。

场景中的点都在同一个平面上，可以使用单应矩阵计算像点的匹配点。

相机的平移距离相对于场景的深度较小的时候，也可以使用单应矩阵。

## <span id = "_Toc75880997" ></span>6.3 Perspective-n-Point

Perspective-n-Point 是在给定一组n个3D 点及其在图像中相应的 2D 投影的情况下，估计校准相机的位姿和内参的问题。

问题的定义：

给定世界参考系中的一组 n 个 3D 点及其相应的 2D 图像投影以及校准的固有相机参数，确定相机相对于世界的旋转和平移的形式确定相机的 6个自由度（6DOF）。根据透视投影模型，有下式

其中，是世界点的齐次坐标。是图像点的齐次坐标。是相机内参矩阵。

是图像点的比例因子。和是正在计算的相机（外部参数）所需的 3D 旋转和 3D 平移，即需要求解的参数。

## <span id = "_Toc75880998" ></span>6.4 LDT

已知：相机内参K ，外参， 三维点P对应的图像点,像素坐标；

求解图像点对应的三维点

相机矩阵

像素坐标;

　　　　;

通过叉乘消去纯量因子，使每个图像点得到三个方程，其中两个是线性独立的，对于第一幅图像有

其中

展开得

其中,是的第i行

同理，对于第二幅图像也可以得到三个方程；

各取两个方程得到四个齐次未知量为四的方程，组成AX=0的方程；

通过SVD解 得到

## <span id = "_Toc75880999" ></span>6.5 参考

Structure-from-Motion Revisited

<https://blog.csdn.net/weixin_44120025/article/details/111085393?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-3.baidujs&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-3.baidujs>

<https://www.cnblogs.com/multhree/p/11296945.html#surf%E7%89%B9%E5%BE%81%E7%AE%80%E4%BB%8B>

<http://hujun1413.github.io/2019/03/09/VSLAM/Colmap/>

<https://blog.csdn.net/u012370185/article/details/84965559>

<https://blog.csdn.net/qq_15642411/article/details/80961291>

<https://blog.csdn.net/qq_33826977/article/details/79834735>

<https://blog.csdn.net/qq_18343569/article/details/48158915>

<https://blog.csdn.net/weixin_44120025/article/details/111085393>

<https://blog.csdn.net/andylanzhiyong/article/details/84778889>

<https://blog.csdn.net/AIchipmunk/article/details/48132109>

<https://blog.csdn.net/mrdonghe/article/details/106147722>

<https://blog.csdn.net/Z5122/article/details/103201716/>

<https://zhuanlan.zhihu.com/p/54197361>

<https://zhuanlan.zhihu.com/p/61742217>

<https://www.zhihu.com/question/27581884>

<https://zhuanlan.zhihu.com/p/138266214>

<https://zhuanlan.zhihu.com/p/33458436>

<https://www.cnblogs.com/youzx/p/6385513.html>

https://www.cnblogs.com/grass-and-moon/p/13637387.html

https://www.cnblogs.com/NEU-2015/p/9915949.html

https://www.cnblogs.com/FangLai-you/p/11276148.html

https://blog.csdn.net/yangwangluoye/article/details/104304537

https://github.com/colmap/colmap

https://github.com/openMVG/ImageDataset_SceauxCastle

<https://en.wikipedia.org/wiki/Random_sample_consensus>

<https://en.wikipedia.org/wiki/Triangulation_(computer_vision)>

<https://en.wikipedia.org/wiki/Kanade%E2%80%93Lucas%E2%80%93Tomasi_feature_tracker>

<https://en.wikipedia.org/wiki/Perspective-n-Point>

<https://en.wikipedia.org/wiki/SFM>

https://en.wikipedia.org/wiki/Bundle_adjustment
